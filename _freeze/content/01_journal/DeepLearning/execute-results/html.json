{
  "hash": "f387123ee0c54b60a27a1c52fca579e8",
  "result": {
    "markdown": "---\ntitle: \"Deep Learning\"\nauthor: \"Fadel Victor Shanaa\"\n---\n\n\n**Challenge Question:**\n\nFor the challenge we are using tabular data instead of images. The goal is to predict customer churn using deep learning with Keras. The objective is similar to the employee churn prediction from the last session.\n\nCustomer churn refers to the situation when a customer ends their relationship with a company, and it’s a costly problem. Customer churn is a problem that all companies need to monitor, especially those that depend on subscription-based revenue streams. Loss of customers impacts sales. We are using the keras package to produce an Artificial Neural Network (ANN) model on the IBM Watson Telco Customer Churn Data Set! As for most business problems, it’s equally important to explain what features drive the model, which is why we’ll use the lime package for explainability. Moreover, we are going to cross-check the LIME results with a Correlation Analysis.\n\nCredit goes to Susan Li.\n\nWe need the following packages:\n\ntidyverse: Loads dplyr, ggplot2 etc. for data wrangling and visualization\nkeras: Ports Keras from Python enabling deep learning in R\nlime: Used to explain the predictions of black box classifiers\nrecipes: package for preprocessing machine learning data sets\nrsample: Package for generating resamples\nyardstick: Tidy methods for measuring model performance\ncorrr: Tidy methods for correlation\n\nAccording to IBM, the business challenge is…\n\nA telecommunications company [Telco] is concerned about the number of customers leaving their landline business for cable competitors. They need to understand who is leaving. Imagine that you’re an analyst at this company and you have to find out who is leaving and why.\n\nThe dataset includes information about:\n\nChurn: Customers who left within the last month\nServices that each customer has signed up for (phone, internet, steaming, … )\nCustomer account information (duration, payment method, … )\nDemographic info about customers (gender, age, … )\n\n\n**Code Base: **\n\n::: {.cell hash='DeepLearning_cache/html/unnamed-chunk-1_ac164fdd7eae68b9d362797017430f7f'}\n\n```{.r .cell-code}\n# Load the libraries\n\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(keras))\nsuppressMessages(library(lime))\nsuppressMessages(library(rsample))\nsuppressMessages(library(recipes))\nsuppressMessages(library(yardstick))\nsuppressMessages(library(corrr))\n\n# Load the data\nchurn_data_raw <- read.csv(\"C:\\\\Users\\\\fvsha\\\\Documents\\\\GitHub\\\\ss23-bdml-VictorReaver1999\\\\deep_learning_challenge\\\\Customer-Churn.csv\")\n\n# View the data\nchurn_data_raw %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 7,043\n#> Columns: 21\n#> $ customerID       <chr> \"7590-VHVEG\", \"5575-GNVDE\", \"3668-QPYBK\", \"7795-CFOCW…\n#> $ gender           <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Female\",…\n#> $ SeniorCitizen    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#> $ Partner          <chr> \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes…\n#> $ Dependents       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\"…\n#> $ tenure           <int> 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 49, 2…\n#> $ PhoneService     <chr> \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", …\n#> $ MultipleLines    <chr> \"No phone service\", \"No\", \"No\", \"No phone service\", \"…\n#> $ InternetService  <chr> \"DSL\", \"DSL\", \"DSL\", \"DSL\", \"Fiber optic\", \"Fiber opt…\n#> $ OnlineSecurity   <chr> \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"…\n#> $ OnlineBackup     <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"N…\n#> $ DeviceProtection <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"Y…\n#> $ TechSupport      <chr> \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Yes…\n#> $ StreamingTV      <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"No\", \"Ye…\n#> $ StreamingMovies  <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes…\n#> $ Contract         <chr> \"Month-to-month\", \"One year\", \"Month-to-month\", \"One …\n#> $ PaperlessBilling <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", …\n#> $ PaymentMethod    <chr> \"Electronic check\", \"Mailed check\", \"Mailed check\", \"…\n#> $ MonthlyCharges   <dbl> 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.7…\n#> $ TotalCharges     <dbl> 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, 1949…\n#> $ Churn            <chr> \"No\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"No\", \"Y…\n```\n:::\n\n```{.r .cell-code}\n# Filter the data\nchurn_data_tbl <- churn_data_raw %>%\n  select(Churn, everything(), -customerID) %>%\n  tidyr::drop_na()\n\n# Split test/training sets\nset.seed(100)\ntrain_test_split <- rsample::initial_split(churn_data_tbl, prop =0.8)\ntrain_test_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> <Training/Testing/Total>\n#> <5625/1407/7032>\n```\n:::\n\n```{.r .cell-code}\n## <Analysis/Assess/Total>\n## <5626/1406/7032>\n\n# Retrieve train and test sets\ntrain_tbl <- training(train_test_split)\ntest_tbl  <- testing(train_test_split)\n\n# Create two plots of tenure counts, one with and the other without binning\ncounts_no_bins <- churn_data_tbl %>% ggplot(aes(x = tenure)) + \n  geom_histogram(binwidth = 0.5, fill =  \"#2DC6D6\") +\n  labs(\n    title = \"Tenure Counts Without Binning\",\n    x     = \"tenure (month)\"\n  )\n\nggsave(\"counts_no_bins.png\", counts_no_bins)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Saving 7 x 5 in image\n```\n:::\n\n```{.r .cell-code}\ncounts_no_bins\n```\n\n::: {.cell-output-display}\n![](DeepLearning_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncounts_6_bins <- churn_data_tbl %>% ggplot(aes(x = tenure)) + \n  geom_histogram(bins = 6, color = \"white\", fill =  \"#2DC6D6\") +\n  labs(\n    title = \"Tenure Counts With Six Bins\",\n    x     = \"tenure (month)\"\n  )\n\nggsave(\"counts_6_bins.png\", counts_6_bins)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Saving 7 x 5 in image\n```\n:::\n\n```{.r .cell-code}\ncounts_6_bins\n```\n\n::: {.cell-output-display}\n![](DeepLearning_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Create a plot of total charges\ntotal_chg_plot <- churn_data_tbl %>% ggplot(aes(x = TotalCharges)) + \n  geom_histogram(bins = 100, fill =  \"#2DC6D6\") +\n  labs(\n    title = \"TotalCharges Histogram, 100 bins\",\n    x     = \"TotalCharges\"\n  )\n\nggsave(\"total_chg_plot.png\", total_chg_plot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Saving 7 x 5 in image\n```\n:::\n\n```{.r .cell-code}\ntotal_chg_plot\n```\n\n::: {.cell-output-display}\n![](DeepLearning_files/figure-html/unnamed-chunk-1-3.png){width=672}\n:::\n\n```{.r .cell-code}\nchurn_data_tbl_mod <- churn_data_tbl %>% \n  mutate(TotalCharges = log10(TotalCharges))\nchurn_data_tbl_mod %>% ggplot(aes(x = TotalCharges)) + \n  geom_histogram(bins = 100, fill =  \"#2DC6D6\") +\n  labs(\n    title = \"TotalCharges Histogram, 100 bins\",\n    x     = \"TotalCharges\"\n  )\n```\n\n::: {.cell-output-display}\n![](DeepLearning_files/figure-html/unnamed-chunk-1-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# Determine if log transformation improves correlation \n# between TotalCharges and Churn\n\ntrain_tbl %>%\n  select(Churn, TotalCharges) %>%\n  mutate(\n    Churn = Churn %>% as.factor() %>% as.numeric(),\n    LogTotalCharges = log(TotalCharges)\n  ) %>%\n  correlate() %>%\n  focus(Churn) %>%\n  fashion()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Correlation computed with\n#> • Method: 'pearson'\n#> • Missing treated using: 'pairwise.complete.obs'\n```\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"noquote\"],\"align\":[\"right\"]},{\"label\":[\"Churn\"],\"name\":[2],\"type\":[\"noquote\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"TotalCharges\",\"2\":\"-.21\"},{\"1\":\"LogTotalCharges\",\"2\":\"-.25\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nchurn_data_tbl %>% \n  pivot_longer(cols      = c(Contract, InternetService, MultipleLines, PaymentMethod), \n               names_to  = \"feature\", \n               values_to = \"category\") %>% \n  ggplot(aes(category)) +\n  geom_bar(fill = \"#2DC6D6\") +\n  facet_wrap(~ feature, scales = \"free\") +\n  labs(\n    title = \"Features with multiple categories: Need to be one-hot encoded\"\n  ) +\n  theme(axis.text.x = element_text(angle = 25, \n                                   hjust = 1))\n```\n\n::: {.cell-output-display}\n![](DeepLearning_files/figure-html/unnamed-chunk-1-5.png){width=672}\n:::\n\n```{.r .cell-code}\n# Create recipe to transform our data\nrec_obj <- recipe(Churn ~ ., data = train_tbl) %>%\n  step_rm(Churn) %>% \n  step_discretize(tenure, options = list(cuts = 6)) %>%\n  step_log(TotalCharges) %>%\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>%\n  step_center(all_predictors(), -all_outcomes()) %>%\n  step_scale(all_predictors(), -all_outcomes()) %>%\n  prep(data = train_tbl)\n\n# Apply the recipe\nx_train_tbl <- bake( rec_obj , new_data =  train_tbl)\nx_test_tbl  <- bake( rec_obj , new_data =  test_tbl)\n\n\ny_train_vec <- ifelse( train_tbl$Churn == \"Yes\", TRUE, FALSE )\ny_test_vec  <- ifelse( test_tbl$Churn  == \"Yes\", TRUE, FALSE)\n\n\n# Building our Artificial Neural Network\nmodel_keras <- keras_model_sequential()\n\nmodel_keras %>% \n  # First hidden layer\n  layer_dense(\n    units              = 16, \n    kernel_initializer = \"uniform\", \n    activation         = \"relu\",\n    input_shape        = ncol(x_train_tbl))%>% \n  # Dropout to prevent overfitting\n  layer_dropout(rate = 0.1) %>%\n  # Second hidden layer\n  layer_dense(\n    units              = 16, \n    kernel_initializer = \"uniform\", \n    activation         = \"relu\") %>% \n  # Dropout to prevent overfitting\n  layer_dropout(rate = 0.1) %>%\n  # Output layer\n  layer_dense(\n    units              = 1, \n    kernel_initializer = \"uniform\", \n    activation         = \"sigmoid\") %>% \n  # Compile ANN\n  compile(\n    optimizer = 'adam',\n    loss      = 'binary_crossentropy',\n    metrics   = c('accuracy')\n  )\n\nmodel_keras\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Model: \"sequential\"\n#> ________________________________________________________________________________\n#>  Layer (type)                       Output Shape                    Param #     \n#> ================================================================================\n#>  dense_2 (Dense)                    (None, 16)                      816         \n#>  dropout_1 (Dropout)                (None, 16)                      0           \n#>  dense_1 (Dense)                    (None, 16)                      272         \n#>  dropout (Dropout)                  (None, 16)                      0           \n#>  dense (Dense)                      (None, 1)                       17          \n#> ================================================================================\n#> Total params: 1,105\n#> Trainable params: 1,105\n#> Non-trainable params: 0\n#> ________________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# I managed to get to this point, but ran into an error that concerns dense_2_input.\n# The rest of the code may be right or wrong, but it's hard to tell without being able to execute it.\n# In theory, it should give the results we need. In view of not being able to train the model, I have\n# decided to stop at this point. The rest of the code will be below, but it's commented out or the rendering and execution will never occur since \n# it keeps failing at the training step\n```\n:::\n\n\n**Code Base Part 2 (Not Working, Commented out): **\n\n\n::: {.cell hash='DeepLearning_cache/html/unnamed-chunk-2_311829342baf2a1502690f72ba1cb49e'}\n\n```{.r .cell-code}\n# x_train_mrx = as.matrix(x_train_tbl)\n# \n# ncol(x_train_tbl)\n# \n# # Fit the model\n# fit_keras <- keras::fit(\n#   object = model_keras,\n#   x = x_train_tbl,\n#   y = y_train_vec ,\n#   epochs = 35 ,\n#   batch_size = 50 ,\n#   validation_split = 0.3\n# )\n# \n# # View fit data\n# fit_keras\n# \n# plot(fit_keras) +\n#   labs(title = \"Deep Learning Training Results\") +\n#   theme(legend.position  = \"bottom\",\n#         strip.placement  = \"inside\",\n#         strip.background = element_rect(fill = \"#grey\"))\n# \n# # Predicted Class\n# yhat_keras_class_vec <- predict_classes(object = model_keras, x = as.matrix(x_test_tbl)) %>%\n#   as.vector()\n# \n# # Predicted Class Probability\n# yhat_keras_prob_vec  <- predict_proba(object = model_keras, x = as.matrix(x_test_tbl)) %>%\n#   as.vector()\n# \n# # Format test data and predictions for yardstick metrics\n# estimates_keras_tbl <- tibble(\n#   truth      = as.factor(y_test_vec) %>% fct_recode(yes = \"1\", no = \"0\"),\n#   estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = \"1\", no = \"0\"),\n#   class_prob = yhat_keras_prob_vec\n# )\n# \n# estimates_keras_tbl\n# \n# # Confusion Table\n# estimates_keras_tbl %>% conf_mat(\n#   truth,\n#   estimate)\n# \n# # Accuracy\n# estimates_keras_tbl %>% accuracy(truth, estimate)\n# \n# # AUC\n# estimates_keras_tbl %>% roc_auc(\n#   data,\n#   truth,\n#   event_level = \"second\")\n# \n# # Precision\n# tibble(\n#   precision = precision(\n#     data,\n#     truth),\n#   recall    = recall(\n#     data,\n#     truth)\n# )\n# \n# # F1-Statistic\n# estimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)\n# \n# class(model_keras)\n# \n# # Setup lime::model_type() function for keras\n# model_type.keras.engine.sequential.Sequential  <- function(x, ...) {\n#   return(\"classification\")\n# }\n# \n# # Setup lime::predict_model() function for keras\n# predict_model.keras.engine.sequential.Sequential <- function(x, newdata, type, ...) {\n#   pred <- predict_proba(object = x, x = as.matrix(newdata))\n#   return(data.frame(Yes = pred, No = 1 - pred))\n# }\n# \n# library(lime)\n# # Test our predict_model() function\n# predict_model(x = model_keras, newdata = x_test_tbl, type = 'raw') %>%\n#   tibble::as_tibble()\n# \n# # Run lime() on training set\n# explainer <- lime::lime(\n#   x_train_tbl,\n#   y_train_vec ,\n#   bin_continuous = FALSE)\n# \n# explanation <- lime::explain(\n#   x_test_tbl[1:10,],\n#   explainer = explainer,\n#   n_labels   = 1,\n#   n_features = 51,\n#   kernel_width   = 1)\n# \n# # Feature correlations to Churn\n# corrr_analysis <- x_train_tbl %>%\n#   mutate(Churn = y_train_vec) %>%\n#   correlate() %>%\n#   focus(Churn) %>%\n#   rename(feature = rowname) %>%\n#   arrange(abs(Churn)) %>%\n#   mutate(feature = as_factor(feature))\n# corrr_analysis\n# \n# # Correlation visualization\n# corrr_plot <- corrr_analysis %>%\n#   ggplot(aes(x = ..., y = fct_reorder(..., desc(...)))) +\n#   geom_point() +\n#   \n#   # Positive Correlations - Contribute to churn\n#   geom_segment(aes(xend = ..., yend = ...),\n#                color = \"red\",\n#                data = corrr_analysis %>% filter(... > ...)) +\n#   geom_point(color = \"red\",\n#              data = corrr_analysis %>% filter(... > ...)) +\n#   \n#   # Negative Correlations - Prevent churn\n#   geom_segment(aes(xend = 0, yend = feature),\n#                color = \"#2DC6D6\",\n#                data = ...) +\n#   geom_point(color = \"#2DC6D6\",\n#              data = ...) +\n#   \n#   # Vertical lines\n#   geom_vline(xintercept = 0, color = \"#f1fa8c\", size = 1, linetype = 2) +\n#   geom_vline( ... ) +\n#   geom_vline( ... ) +\n#   \n#   # Aesthetics\n#   labs( ... )\n# \n# # Save the plot\n# ggsave(\"corrr_plot.png\", corrr_plot)\n# \n# # View the plot\n# corrr_plot\n```\n:::\n\n\n**End of Deep Learning Challenge and BDML Course**",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}